{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1058fdf9",
   "metadata": {},
   "source": [
    "Text Preprocessing is a second step of NLP Pipeline and it is very much important step as it involve analysing of data in the initial stage. \n",
    "It is generally of 2 type:\n",
    "1. Basic\n",
    "2. Advance\n",
    "\n",
    "However in this code we will be focusing on Basic type :\n",
    "Following Techinque we will see in basis text pre-processing\n",
    " - LowerCasing\n",
    " - Removing HTML Tags\n",
    " - Removing URL \n",
    " - Removing Punctuations\n",
    " - Chatwords Treatment\n",
    " - Spelling Correction\n",
    " - Removing Stopwords\n",
    " - Handling Emojis\n",
    " - Tokenization\n",
    " - Stemming \n",
    " - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e352f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e54260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16459e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04446f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fe96b",
   "metadata": {},
   "source": [
    "### 1. Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435a2172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. <br /><br />the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. <br /><br />the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower casing a particular review\n",
    "\n",
    "df.review[1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LowerCasing the entire corpus\n",
    "\n",
    "df.review = df.review.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385fc493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0119d7",
   "metadata": {},
   "source": [
    "### 2.Removing Html Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773e8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Html tags are use to help the browser in displaying the data.However while doing or working on sentimental \n",
    "# analysis we donot require these tags.So we use Regular expression to remove those tags and create patterns\n",
    "\n",
    "# General pattern to find any Html tag is '<.*?>'\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df.review = df.review.apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4a33b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. the filming tec...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea755691",
   "metadata": {},
   "source": [
    "### 3. Removing Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda1697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While working with Social Media data, we get lot lot of URL and it better to remove these URL's \n",
    "# We again use regular expression to remove these URL\n",
    "# General Pattern follows > r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "# We don't have any URL in our data, to implement it we use some demo data\n",
    "\n",
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b6aef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1 : Check out my notebook \n",
      "Text2 : Check out my notebook \n",
      "Text3 : Google search here \n",
      "Text4 : For notebook click  to search check \n"
     ]
    }
   ],
   "source": [
    "# Checking the results\n",
    "\n",
    "print('Text1 :', remove_url(text1))\n",
    "print('Text2 :', remove_url(text2))\n",
    "print('Text3 :', remove_url(text3))\n",
    "print('Text4 :', remove_url(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4d31b",
   "metadata": {},
   "source": [
    "### 4. Removing the Punctuation Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bdff75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Punctuation are important to be removed as they may cause unnecessary complexity to the model during\n",
    "# tokkenization, as Hello! or Hello may be consider as different word and ! can also be consider as 1 work which\n",
    "# may increase the complexity of the Model by adding more words\n",
    "\n",
    "# We use string, time library,\n",
    "# string.punctution give the set of all the punctuation we have in the python\n",
    "\n",
    "import string, time\n",
    "\n",
    "punct = string.punctuation\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f401c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string with Punctuation'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    for i in punct:\n",
    "        text = text.replace(i,'')\n",
    "    return text\n",
    "\n",
    "text1 = 'string. with. Punctuation?'\n",
    "\n",
    "remove_punctuation(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63d8405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011491775512695312\n"
     ]
    }
   ],
   "source": [
    "# Those we have achieve our goal however there is an issue with this code.\n",
    "# Time taken by this code is high\n",
    "\n",
    "start = time.time()\n",
    "remove_punctuation(text1)\n",
    "time1 = time.time()-start\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6effd0",
   "metadata": {},
   "source": [
    "Since we are performing on single text that why is showing low.However if we to apply this over 50k rows then\n",
    "its will be close to 67sec, more than a minute\n",
    "So we write different code for the same which is more faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e60dd591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013399124145507812\n"
     ]
    }
   ],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','', punct))\n",
    "\n",
    "start1 = time.time()\n",
    "remove_punc1(text1)\n",
    "time2 = time.time()-start1\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c580161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.745887756347656 3.3020973205566406\n"
     ]
    }
   ],
   "source": [
    "print(time1*50000, time2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b354d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8576512455516014"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time1/time2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f205e",
   "metadata": {},
   "source": [
    "So the results show its is much fater than earlier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17277c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets apply to this to a differnet dataset from Kaggle\n",
    "\n",
    "df1 = pd.read_csv('labeled_data.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc41e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45d906ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         RT mayasolovely As a woman you shouldnt compl...\n",
       "1         RT mleew17 boy dats coldtyga dwn bad for cuff...\n",
       "2         RT UrKindOfBrand Dawg RT 80sbaby4life You eve...\n",
       "3           RT CGAnderson vivabased she look like a tranny\n",
       "4         RT ShenikaRoberts The shit you hear about me ...\n",
       "                               ...                        \n",
       "24778    yous a muthafin lie 8220LifeAsKing 20Pearls co...\n",
       "24779    youve gone and broke the wrong heart baby and ...\n",
       "24780    young buck wanna eat dat nigguh like I aint fu...\n",
       "24781                youu got wild bitches tellin you lies\n",
       "24782    Ruffled  Ntac Eileen Dahlia  Beautiful color c...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying on tweet column \n",
    "\n",
    "df1.tweet.apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfcfa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And it works faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85383ca6",
   "metadata": {},
   "source": [
    "### 5. Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d42e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatword we use in day to day social media need a proper treatment while doing a sentimental analysis.\n",
    "# Word like , asap - As soon as possible, gn- good night and other need to be defined properly.\n",
    "# We just a need define the dictionay to with the fullforms and then replace then in our text\n",
    "\n",
    "chat_word = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK' : 'Away From Keyboard',\n",
    "    'ASAP':'As Soon As Possible',\n",
    "'ATK':'At The Keyboard',\n",
    "'ATM' :'At The Moment',\n",
    "'A3': 'Anytime, Anywhere, Anyplace',\n",
    "'BAK': 'Back At Keyboard',\n",
    "'BBL' : 'Be Back Later',\n",
    " 'BBS':'Be Back Soon',\n",
    "'BFN':'Bye For Now',\n",
    "'B4N':'Bye For Now',\n",
    "'BRB':'Be Right Back',\n",
    "'BRT': 'Be Right There',\n",
    "'BTW':'By The Way',\n",
    "'B4' :'Before',\n",
    "'B4N' :'Bye For Now',\n",
    "'CU':'See You',\n",
    "'CUL8R': 'See You Later',\n",
    "'CYA' : 'See You',\n",
    "'FAQ' :'Frequently Asked Questions',\n",
    "'FC':'Fingers Crossed',\n",
    "'FWIW': \"For What It's Worth\",\n",
    "'FYI': 'For Your Information',\n",
    "'GAL' :'Get A Life',\n",
    "'GG':'Good Game',\n",
    "'GN':'Good Night',\n",
    "'GMTA': 'Great Minds Think Alike',\n",
    "'GR8' :'Great!',\n",
    "'G9':'Genius',\n",
    "'IC':'I See',\n",
    "'ICQ':'I Seek you (also a chat program)',\n",
    "'ILU' :'ILU: I Love You',\n",
    "'IMHO':'In My Honest/Humble Opinion',\n",
    "'IMO' :'In My Opinion',\n",
    "'IOW':'In Other Words',\n",
    "'IRL' :'In Real Life',\n",
    "'KISS':'Keep It Simple, Stupid',\n",
    "'LDR':'Long Distance Relationship',\n",
    "'LMAO':'Laugh My A.. Off',\n",
    "'LOL':'Laughing Out Loud',\n",
    "'LTNS' : 'Long Time No See',\n",
    "'L8R' :'Later',\n",
    "'MTE':'My Thoughts Exactly',\n",
    "'M8' :'Mate',\n",
    "'NRN' :'No Reply Necessary',\n",
    "'OIC':'Oh I See',\n",
    "'PITA' :'Pain In The A..',\n",
    "'PRT' :'Party',\n",
    "'PRW':'Parents Are Watching',\n",
    "'QPSA' :'Que Pasa',\n",
    "'ROFL':'Rolling On The Floor Laughing',\n",
    "'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "'ROTFLMAO':'Rolling On The Floor Laughing My A.. Off',\n",
    "'SK8':'Skate',\n",
    "'STATS':'Your sex and age',\n",
    "'ASL':'Age, Sex, Location',\n",
    "'THX':'Thank You',\n",
    "'TTFN':'Ta-Ta For Now!',\n",
    "'TTYL':'Talk To You Later',\n",
    "'U':'You',\n",
    "'U2':'You Too',\n",
    "'U4E':'Yours For Ever',\n",
    "'WB':'Welcome Back',\n",
    "'WTF':'What The Fuck',\n",
    "'WTG':'Way To Go!',\n",
    "'WUF':'Where Are You From?',\n",
    "'W8':'Wait...',\n",
    "'MFW':'My face when',\n",
    "'MRW':'My reaction when',\n",
    "'IFYP':'I feel your pain',\n",
    "'LOL':'Laughing out loud',\n",
    "'TNTL':'Trying not to laugh',\n",
    "'JK':'Just kidding',\n",
    "'IDC':'I don‚Äôt care',\n",
    "'ILY':'I love you',\n",
    "'IMU':'I miss you',\n",
    "'ADIH':'Another day in hell',\n",
    "'IDC':'I don‚Äôt care',\n",
    "'ZZZ':'Sleeping, bored, tired',\n",
    "'WYWH':'Wish you were here',\n",
    "'TIME' :'Tears in my eyes',\n",
    "'BAE': 'Before anyone else',\n",
    "'FIMH': 'Forever in my heart',\n",
    "'BSAAW': 'Big smile and a wink',\n",
    "'BWL': 'Bursting with laughter',\n",
    "'LMAO': 'Laughing my a** off',\n",
    "'BFF': 'Best friends forever',\n",
    "'CSL' : 'Can‚Äôt stop laughing'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb6b8f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is the Best!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the a function\n",
    "\n",
    "def chat_correction(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_word:\n",
    "            new_text.append(chat_word[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "chat_correction('IMHO he is the Best!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMHO is replaced with In My Honest/Humble Opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9217f03",
   "metadata": {},
   "source": [
    "### 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "091641cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certain conditions during several generations are modified in the same manner.\n"
     ]
    }
   ],
   "source": [
    "# Spelling correction is also important due to the same propuse to avoid any kind of complexity during tokenization\n",
    "# As words with same meaning and different spelling unnecessary increase the complexity of the model.\n",
    "# There are different techniques to follow, We can either use NLTK library or textblobs library\n",
    "# Here we will see it using textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "\n",
    "# create a TextBlob obj\n",
    "\n",
    "txt_blob = TextBlob(incorrect_text)\n",
    "\n",
    "# To print the correct sting, we will use textblob.correct() function\n",
    "\n",
    "print(txt_blob.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aae39",
   "metadata": {},
   "source": [
    "TextBlob is helpful is making the spell check for normal words however while dealing with complex we may have to \n",
    "create our own spell checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703862a",
   "metadata": {},
   "source": [
    "### 7.Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3a03462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words are the words which are used for sentence formation but they don't have any actual meaning hence\n",
    "# while dealing with sentimental analysis these words need to be removed to avoid any complexity\n",
    "# Exception to that would be while doing POS(Parts of Speech) tagging we DONOT remove Stop Word\n",
    "# We use NLTK library to perform this task as it consist of build-in list for stop words used in English and other\n",
    "# Languages\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1810f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "sample = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "\n",
    "remove_stopwords(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950885c6",
   "metadata": {},
   "source": [
    "As we observed, many stop words are removed like my, a ,of..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10b50dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        !!! RT @mayasolovely: As  woman   complain  cl...\n",
       "1        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3        !!!!!!!!! RT @C_G_Anderson: @viva_based  look ...\n",
       "4        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit  he...\n",
       "                               ...                        \n",
       "24778    you's  muthaf***in lie &#8220;@LifeAsKing: @20...\n",
       "24779     gone  broke  wrong heart baby,  drove  rednec...\n",
       "24780    young buck wanna eat!!.. dat nigguh like I ain...\n",
       "24781                   youu got wild bitches tellin  lies\n",
       "24782    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying on dataset\n",
    "\n",
    "df1.tweet.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6957c",
   "metadata": {},
   "source": [
    "### 8. Handing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35eca759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emojis has change the revolution of how people express themselves and they are important to handle\n",
    "# Handling can be done either by removing the emojis or replacing them by their meaning\n",
    "# We us regular expression perform the tast and the code can be use as snippet\n",
    "\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b81d9890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was üòòüòò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f44635f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Lmao üòÇüòÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2077d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-2.8.0\n"
     ]
    }
   ],
   "source": [
    "# For Replacing the emojis we use 'emoji' libraries from the python\n",
    "\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af75172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# to replace the emoji with it meaning we use demojize()\n",
    "\n",
    "print(emoji.demojize('Python is üî•'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94af2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Loved the movie. It was üòò'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f0157",
   "metadata": {},
   "source": [
    "### 9.Tokkenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25a75fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is nothing by splitting the raw text into small chunks of words or sentences, called tokens\n",
    "# It is a very crucial and importance step of text pre-processing without doing proper tokenization the model\n",
    "# may fail to perform\n",
    "# It can be of Word Tokenization, Sentence Tokenization, Phrase Tokenization\n",
    "# Tokenization and be done in multiple ways\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf2f42",
   "metadata": {},
   "source": [
    "### 9(a). Using the Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0db57e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Jammu']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "sent1 = 'I am going to Jammu'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9502fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to Delhi',\n",
       " 'I will stay there for a week',\n",
       " \"Let's hope the trip goes well!\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenizaton\n",
    "\n",
    "sent2 = 'I am going to Delhi.I will stay there for a week.Let\\'s hope the trip goes well!'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b0b263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi!']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem with split function\n",
    "\n",
    "sent3 = 'I am going to Delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f902545",
   "metadata": {},
   "source": [
    "! come along with Delhi which is suppose to be treated separately and which will be different from Delhi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f184d",
   "metadata": {},
   "source": [
    "### 9(b). Using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1504da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "\n",
    "import re\n",
    "sent3 = 'I am going to Delhi!'\n",
    "tokens = re.findall(\"[\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e5684",
   "metadata": {},
   "source": [
    "Its better than split function but creating patterns everytime could be tedious task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2dcab5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851bf75",
   "metadata": {},
   "source": [
    "### 9(c). Using NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f92acfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi', '!']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better option to perform tokenization is to use libraries which has built in function and gives better results\n",
    "# than split() function and regular expression\n",
    "# We have NLTK library which where we have 2 functions word_tokenize and sent_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#word tokenization\n",
    "    \n",
    "sent1 = 'I am going to Delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78331126",
   "metadata": {},
   "source": [
    "Better results than above techniques as ! is also separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c0f74bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence tokenization\n",
    "\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbf2f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I'] \n",
      " ['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com'] \n",
      " ['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "#Other eg for word tokkenization\n",
    "\n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "print(word_tokenize(sent5),'\\n',word_tokenize(sent6),'\\n',word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0625936",
   "metadata": {},
   "source": [
    "For send6 it failed as it split the email nks@gmail.com also so NLTK also has issues but still shows better result than regular expression and split() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9c8f2",
   "metadata": {},
   "source": [
    "### 9(d). Using Spacy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33f63c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spcay can also be use to perform the tokenization and give good results as compare to nltk\n",
    "# we need to load 'en_core_web_sm' small dictonary to perform the task\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5819f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting text into document\n",
    "\n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9111e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "# Looping over each document to check the word_tokenization\n",
    "for i in doc1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0545265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for i in doc2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e02c152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for i in doc3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e235a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "Delhi\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in doc4:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd286427",
   "metadata": {},
   "source": [
    "As observed the results are better than Nltk and other techniques so we can use the desired techinques based upon our requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bf280",
   "metadata": {},
   "source": [
    "### 10. Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bd735ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming, in Natural Language Processing (NLP), refers to the process of reducing a word to its word stem \n",
    "# that affixes to suffixes and prefixes or the roots.\n",
    "# Basically the process of reducing the inflected words from our data.\n",
    "\n",
    "# Stemming can be performed using NLTk Library using PortorStemmer function\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    new_txt = []\n",
    "    for word in text.split():\n",
    "        new_txt.append(ps.stem(word)) # ps.stem()> will convert inflected working into it root form\n",
    "    \n",
    "    return \" \".join(new_txt) \n",
    "\n",
    "sample = 'walks walking walked walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e7f6734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ae4f0",
   "metadata": {},
   "source": [
    "As we can observe, that the stemming changes the words to their root form however it is not necessary that they have meaning in that language like in above eg, probably becomes probabl, story becomes stori. So if we need to show this output to someone then stemming is not good and from their \"lemmitization\" comes into picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9965d",
   "metadata": {},
   "source": [
    "### 11. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "892e679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He >> He\n",
      "was >> be\n",
      "running >> run\n",
      "and >> and\n",
      "eating >> eat\n",
      "at >> at\n",
      "same >> same\n",
      "time >> time\n",
      "He >> He\n",
      "has >> have\n",
      "bad >> bad\n",
      "habit >> habit\n",
      "of >> of\n",
      "swimming >> swim\n",
      "after >> after\n",
      "playing >> play\n",
      "long >> long\n",
      "hours >> hours\n",
      "in >> in\n",
      "the >> the\n",
      "Sun >> Sun\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word \n",
    "# down to its root meaning to identify similarities. \n",
    "# Lemmatization is search technique which search the words lexican dictionary which consist of relations between\n",
    "# different words in a language and returns a meaningful root words of same langauge.\n",
    "# Lemmatization is slower than stemming and is mainly use when the output need to be displayed\n",
    "# We use nltk library which consit of WordNetLemmatizer to perform Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sent = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "# performing tokenization\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sent)\n",
    "\n",
    "# removing the puctuations\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "# performing the lemmatization and comparing the output\n",
    "# To perform the lemmatization we us .lemmatize(text, pos)\n",
    "# pos variable we need to define as to which part of speech of the text we need to perform lemmatization\n",
    "# Valid options are ‚Äún‚Äù for nouns, ‚Äúv‚Äù for verbs, ‚Äúa‚Äù for adjectives, ‚Äúr‚Äù for adverbs \n",
    "# and ‚Äús‚Äù for satellite adjectives.\n",
    "\n",
    "\n",
    "for i in sentence_words:\n",
    "    print(f\"{i} >> {lemmatizer.lemmatize(i, pos='v')}\" ) # Applying Lemmatization on verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d161cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334febf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a98e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315c355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e07ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18912e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60733b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
