{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a372eef2",
   "metadata": {},
   "source": [
    "Feature Extraction from Text / Text Vectorization / Text Representation is the process of converting text into numerical representation.\n",
    "There are different techniques to do that ML Approach and DP Appoach\n",
    "In this code we will looking in to 3 techniques\n",
    "- Bag of Words\n",
    "- Ngram\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c4e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfaf01e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "df = pd.DataFrame({'text':['people watch campusx', 'campusx watch campusx', 'people write comment', 'campusx write comment'],\n",
    "                  'output':[1,1,0,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170e488",
   "metadata": {},
   "source": [
    "## 1.Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a22d241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words(BoW) is a statistical language model used to analyze text and documents based on word count.\n",
    "# To implement BOW we use sklearn.feature_extraction library and from there we use CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#creating Countvectorizer object\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fe66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "# To implement the BOW on the df we use .fit_trasnform and then pass the column which on which we want to perform\n",
    "# BOW, Here we will implement it on 'text' column\n",
    "\n",
    "bow = cv.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32a40862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36f744",
   "metadata": {},
   "source": [
    "bow is matrix type to view the consent of bow we need to convert it into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45024268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0],\n",
       "       [2, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642c6d0",
   "metadata": {},
   "source": [
    "So it s (4,5) vector with each row encoded as per the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62a2682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the vocalubary we use cv.vocabulary_\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424840d3",
   "metadata": {},
   "source": [
    "As observe vocabulary for 5 words in acsending order is created.\n",
    "- campusx : 0 index\n",
    "- comment : 1 index\n",
    "- people : 2 index\n",
    "- watch : 3 index\n",
    "- write : 4 index\n",
    "\n",
    "Now each row will be encoded on the basis of this only,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8803626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the encoding of row 1 : 'people watch campusx'\n",
    "bow[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8d9b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the prediction for another text, we use .transform and then pass that text and then convert it into \n",
    "# array to see the encoding.\n",
    "\n",
    "cv.transform(['campusx watch and write comment of campusx']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d0f9d",
   "metadata": {},
   "source": [
    "Here we can observe that and and of were not the part of vocalbulary still the input size is same that is because\n",
    "the algorithm is desined in such a way that the encoding done on the basis of vocaulary and we aren't consider the any word which are out of vocab. so this resolve the OOV issue which we were facing in OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905393e2",
   "metadata": {},
   "source": [
    "### Hypertunning of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7712d59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are multiple parameters in CountVectorizer as mentioned below and its a very powerful function\n",
    "\n",
    "# class sklearn.feature_extraction.text.CountVectorizer(*, input='content', \n",
    "#                                                       encoding='utf-8', \n",
    "#                                                       decode_error='strict', \n",
    "#                                                       strip_accents=None, \n",
    "#                                                       lowercase=True, \n",
    "#                                                       preprocessor=None, \n",
    "#                                                       tokenizer=None, \n",
    "#                                                       stop_words=None, \n",
    "#                                                       token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "#                                                       ngram_range=(1, 1), \n",
    "#                                                       analyzer='word', \n",
    "#                                                       max_df=1.0, min_df=1, \n",
    "#                                                       max_features=None, \n",
    "#                                                       vocabulary=None, \n",
    "#                                                       binary=False, \n",
    "#                                                       dtype=<class 'numpy.int64'>)\n",
    "\n",
    "# max_features=None \n",
    "# This parameter is basically used to remove the rare words and encoding is done on the basis of word with max\n",
    "# frequency, lets say we give max_feature=1, that means, the word in the vocabulary with max frequency in corpus\n",
    "# will be use for encoding. \n",
    "# In above eg, campusx has highest frequency of 4 so giving max_feaure=1 will encode the data on the basis of \n",
    "# campusx only.\n",
    "\n",
    "cv1 = CountVectorizer(max_features=1)\n",
    "bow1 = cv1.fit_transform(df.text).toarray()\n",
    "bow1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2de96",
   "metadata": {},
   "source": [
    "As we observed document 3, or row 3 doesn't have any campusx in it so we it has been encoded as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f9ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary = False\n",
    "# This feature is use to set all Non-Zero to 1. This is basically used when the occurance of the a particular \n",
    "# word matter more than its frequncy\n",
    "# So setting binary=True , if a particular word is occurring more than 1 in a Document, it will still be encoded \n",
    "# as 1.\n",
    "\n",
    "cv2 = CountVectorizer(binary=True)\n",
    "bow2 = cv2.fit_transform(df.text).toarray()\n",
    "bow2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a630b5",
   "metadata": {},
   "source": [
    "As observed, in 2nd row, campusx was occuring 2 times however its still encoded as 1 coz we set binary=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2307910",
   "metadata": {},
   "source": [
    "## 2.Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams are continuous sequences of words or symbols, or tokens in a document\n",
    "# In N-gram,the vocabulary is created by using more than 1 word.\n",
    "# Bi-gram if we are creating vocabulary using 2 continous words\n",
    "# Tri-gram if we are creating vocabulary using 3 continous words\n",
    "\n",
    "# For implemetation of N-gram , we use hyper parameter of CountVectorizer with name ngram_range\n",
    "# where we pass a tupple with size, \n",
    "# for uniram ~ BOW = (1,1)\n",
    "# for Bigram = (2,2)\n",
    "# for Trigram = (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b93951ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram\n",
    "\n",
    "cv3 = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bbff104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram vector\n",
    "bigram = cv3.fit_transform(df.text).toarray()\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e043d9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 2,\n",
       " 'watch campusx': 4,\n",
       " 'campusx watch': 0,\n",
       " 'people write': 3,\n",
       " 'write comment': 5,\n",
       " 'campusx write': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking bi-gram vocabulary\n",
    "\n",
    "cv3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b47725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigram\n",
    "\n",
    "cv4 = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "# Trigram Vector\n",
    "\n",
    "trigram = cv4.fit_transform(df.text).toarray()\n",
    "\n",
    "trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b16e37d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch campusx': 2,\n",
       " 'campusx watch campusx': 0,\n",
       " 'people write comment': 3,\n",
       " 'campusx write comment': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigram Vocabulary\n",
    "\n",
    "cv4.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "413ba929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
       "       [2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Special cases\n",
    "# Now instead of passing (1,1), (2,2) and (3,3) as ngram_range if we pass \n",
    "# (1,2) - This will create the vocabulary for unigram + bigram so in total we will have 11 word in vocabulary\n",
    "\n",
    "# Combination of Uni-gram and Bi-Gram\n",
    "\n",
    "cv5 = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Combine Vec\n",
    "\n",
    "comb_vec = cv5.fit_transform(df.text).toarray()\n",
    "\n",
    "comb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "025e6544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 4,\n",
       " 'watch': 7,\n",
       " 'campusx': 0,\n",
       " 'people watch': 5,\n",
       " 'watch campusx': 8,\n",
       " 'campusx watch': 1,\n",
       " 'write': 9,\n",
       " 'comment': 3,\n",
       " 'people write': 6,\n",
       " 'write comment': 10,\n",
       " 'campusx write': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Vorct Vocalbulary\n",
    "\n",
    "cv5.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409440b",
   "metadata": {},
   "source": [
    "As per can see we have combine vocabulary for Uni-gram and Bi-gram the resultant shape is (4,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b063d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination of Uni, Bi, and Tri Gram\n",
    "\n",
    "cv6 = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Combine vec\n",
    "\n",
    "comb_vec1 = cv6.fit_transform(df.text).toarray()\n",
    "\n",
    "comb_vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ec4b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 6,\n",
       " 'watch': 11,\n",
       " 'campusx': 0,\n",
       " 'people watch': 7,\n",
       " 'watch campusx': 12,\n",
       " 'people watch campusx': 8,\n",
       " 'campusx watch': 1,\n",
       " 'campusx watch campusx': 2,\n",
       " 'write': 13,\n",
       " 'comment': 5,\n",
       " 'people write': 9,\n",
       " 'write comment': 14,\n",
       " 'people write comment': 10,\n",
       " 'campusx write': 3,\n",
       " 'campusx write comment': 4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CombineVec Library\n",
    "\n",
    "cv6.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be5696",
   "metadata": {},
   "source": [
    "As we can see we have combination of all uni,bi,tri gram vocabulary and resultant shape is (4,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b40086",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If We try to move further lets say with Quard-Gram, the system will show error as we donot have 4 continues words\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# in any of the rows to create the vocabulary.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m cv7 \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m quard_gram \u001b[38;5;241m=\u001b[39m cv7\u001b[38;5;241m.\u001b[39mfit_transform(df\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1291\u001b[0m         )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# If We try to move further lets say with Quard-Gram, the system will show error as we donot have 4 continues words\n",
    "# in any of the rows to create the vocabulary.\n",
    "\n",
    "cv7 = CountVectorizer(ngram_range=(4,4))\n",
    "\n",
    "quard_gram = cv7.fit_transform(df.text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a418a",
   "metadata": {},
   "source": [
    "As observe, python is giving error as empty vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c2e01",
   "metadata": {},
   "source": [
    "## 3.TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency - Inverse Document Frequency (TF-IDF) is a widely used statistical method in NLP and information \n",
    "# retrieval. It measures how important a term is within a document relative to a collection of documents \n",
    "\n",
    "# TF-IDF basically focus on the weightage of a particular word/term in a given document. \n",
    "# Weightage('t') = T.F('t' in give 'D') * IDF('t')\n",
    "# We don't need to go into calculation part its build-in\n",
    "# We use sklearn.feature_extraction.text.TfidfVectorizer to perform this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0740ea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49681612, 0.        , 0.61366674, 0.61366674, 0.        ],\n",
       "       [0.8508161 , 0.        , 0.        , 0.52546357, 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.57735027],\n",
       "       [0.49681612, 0.61366674, 0.        , 0.        , 0.61366674]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating td_idf object\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "# To create the vector we use similar approach as CountVectorizer.fit_transform which gives us a spare matrix\n",
    "# and which we can convert into array using .toarray()\n",
    "\n",
    "vec = tf_idf.fit_transform(df.text).toarray()\n",
    "\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b5061a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22314355, 1.51082562, 1.51082562, 1.51082562, 1.51082562])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since IDF values are constant we can review then using obj.idf_\n",
    "\n",
    "tf_idf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86b90b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the model Vocabulary\n",
    "\n",
    "tf_idf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b65874",
   "metadata": {},
   "source": [
    "As observed, Vocabulary of TD-IDF is similar to BOW, only differnce is we are focusing on weightage here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdf230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889f9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7defb130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1f6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
